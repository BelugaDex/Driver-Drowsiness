{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c1716e8-6408-4346-87a2-82a5759511d0",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torch) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "264dd202-a1f4-4b05-8d1c-5e9dd39c595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "763d1dd7-5115-4334-b217-0ebb7e474a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitpython>=3.1.30 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (3.1.43)\n",
      "Requirement already satisfied: matplotlib>=3.3 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 6)) (3.9.2)\n",
      "Requirement already satisfied: numpy>=1.23.5 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n",
      "Requirement already satisfied: pillow>=10.3.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 9)) (10.3.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 10)) (6.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 11)) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 15)) (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 16)) (0.21.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 17)) (4.66.5)\n",
      "Requirement already satisfied: ultralytics>=8.2.34 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 18)) (8.3.94)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 27)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 42)) (77.0.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.32.2->-r requirements.txt (line 12)) (2024.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->-r requirements.txt (line 17)) (0.4.6)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.14)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2022.7.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2023.3)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\biswarajdey\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 & pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132fddc5-75e0-48c5-8cd4-8fe9c9777e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall backports -y\n",
    "#!pip install backports.tarfile --no-cache-dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83987110-23cb-430c-b9bd-af32022434ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade setuptools pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73863899-c47a-4953-9d84-4a9fef56e451",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import uuid #unique identifier\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd1eb8f0-cb60-4e66-9ab0-90248dfed583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting images for awake\n",
      "Collecting images for awake, image number 0\n",
      "Collecting images for awake, image number 1\n",
      "Collecting images for awake, image number 2\n",
      "Collecting images for awake, image number 3\n",
      "Collecting images for awake, image number 4\n",
      "Collecting images for awake, image number 5\n",
      "Collecting images for awake, image number 6\n",
      "Collecting images for awake, image number 7\n",
      "Collecting images for awake, image number 8\n",
      "Collecting images for awake, image number 9\n",
      "Collecting images for awake, image number 10\n",
      "Collecting images for awake, image number 11\n",
      "Collecting images for awake, image number 12\n",
      "Collecting images for awake, image number 13\n",
      "Collecting images for awake, image number 14\n",
      "Collecting images for awake, image number 15\n",
      "Collecting images for awake, image number 16\n",
      "Collecting images for awake, image number 17\n",
      "Collecting images for awake, image number 18\n",
      "Collecting images for awake, image number 19\n",
      "Collecting images for drowsy\n",
      "Collecting images for drowsy, image number 0\n",
      "Collecting images for drowsy, image number 1\n",
      "Collecting images for drowsy, image number 2\n",
      "Collecting images for drowsy, image number 3\n",
      "Collecting images for drowsy, image number 4\n",
      "Collecting images for drowsy, image number 5\n",
      "Collecting images for drowsy, image number 6\n",
      "Collecting images for drowsy, image number 7\n",
      "Collecting images for drowsy, image number 8\n",
      "Collecting images for drowsy, image number 9\n",
      "Collecting images for drowsy, image number 10\n",
      "Collecting images for drowsy, image number 11\n",
      "Collecting images for drowsy, image number 12\n",
      "Collecting images for drowsy, image number 13\n",
      "Collecting images for drowsy, image number 14\n",
      "Collecting images for drowsy, image number 15\n",
      "Collecting images for drowsy, image number 16\n",
      "Collecting images for drowsy, image number 17\n",
      "Collecting images for drowsy, image number 18\n",
      "Collecting images for drowsy, image number 19\n"
     ]
    }
   ],
   "source": [
    "images_path=os.path.join('data','images') #data/images\n",
    "labels = ['awake','drowsy']\n",
    "number_imgs= 20\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Loop through labels\n",
    "for label in labels:\n",
    "    print('Collecting images for {}'.format(label))\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Loop through image range\n",
    "    for img_num in range(number_imgs):\n",
    "        print('Collecting images for {}, image number {}'.format(label, img_num))\n",
    "        \n",
    "        # Webcam feed\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Naming out image path\n",
    "        imgname = os.path.join(images_path, label+'.'+str(uuid.uuid1())+'.jpg')\n",
    "        \n",
    "        # Writes out image to file \n",
    "        cv2.imwrite(imgname, frame)\n",
    "        \n",
    "        # Render to the screen\n",
    "        cv2.imshow('Image Collection', frame)\n",
    "        \n",
    "        # 2 second delay between captures\n",
    "        time.sleep(2)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c41609-2cd3-4f41-920c-5692dd3cee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'labelImg'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/HumanSignal/labelImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6586b6d-8f4d-4822-9689-487e779143f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyqt5 in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (5.15.10)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\BiswarajDey\\anaconda3\\Lib\\site-packages\\~xml'.\n",
      "  You can safely remove it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pyqt5\n",
      "  Using cached PyQt5-5.15.11-cp38-abi3-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: lxml in c:\\users\\biswarajdey\\anaconda3\\lib\\site-packages (5.2.1)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl.metadata (3.8 kB)\n",
      "Collecting PyQt5-sip<13,>=12.15 (from pyqt5)\n",
      "  Downloading PyQt5_sip-12.17.0-cp312-cp312-win_amd64.whl.metadata (492 bytes)\n",
      "Collecting PyQt5-Qt5<5.16.0,>=5.15.2 (from pyqt5)\n",
      "  Using cached PyQt5_Qt5-5.15.2-py3-none-win_amd64.whl.metadata (552 bytes)\n",
      "Using cached PyQt5-5.15.11-cp38-abi3-win_amd64.whl (6.9 MB)\n",
      "Downloading lxml-5.3.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 381.6 kB/s eta 0:00:09\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   -------- ------------------------------- 0.8/3.8 MB 211.1 kB/s eta 0:00:15\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 174.8 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 1.3/3.8 MB 184.9 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 1.3/3.8 MB 184.9 kB/s eta 0:00:14\n",
      "   ------------- -------------------------- 1.3/3.8 MB 184.9 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 204.1 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 204.1 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 204.1 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 204.1 kB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 1.6/3.8 MB 204.1 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 1.8/3.8 MB 214.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 1.8/3.8 MB 214.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 1.8/3.8 MB 214.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 1.8/3.8 MB 214.2 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 2.1/3.8 MB 221.6 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 2.1/3.8 MB 221.6 kB/s eta 0:00:08\n",
      "   --------------------- ------------------ 2.1/3.8 MB 221.6 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 2.4/3.8 MB 238.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.4/3.8 MB 238.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.4/3.8 MB 238.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.4/3.8 MB 238.4 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.6/3.8 MB 242.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 2.6/3.8 MB 242.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 2.6/3.8 MB 242.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 2.6/3.8 MB 242.0 kB/s eta 0:00:05\n",
      "   --------------------------- ------------ 2.6/3.8 MB 242.0 kB/s eta 0:00:05\n",
      "   ------------------------------ --------- 2.9/3.8 MB 247.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 2.9/3.8 MB 247.1 kB/s eta 0:00:04\n",
      "   ------------------------------ --------- 2.9/3.8 MB 247.1 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 3.1/3.8 MB 255.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 3.1/3.8 MB 255.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 3.1/3.8 MB 255.6 kB/s eta 0:00:03\n",
      "   -------------------------------- ------- 3.1/3.8 MB 255.6 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 260.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 260.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 260.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 3.4/3.8 MB 260.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.7/3.8 MB 261.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 261.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 261.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 261.5 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 3.7/3.8 MB 261.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 253.4 kB/s eta 0:00:00\n",
      "Using cached PyQt5_Qt5-5.15.2-py3-none-win_amd64.whl (50.1 MB)\n",
      "Downloading PyQt5_sip-12.17.0-cp312-cp312-win_amd64.whl (58 kB)\n",
      "Installing collected packages: PyQt5-Qt5, PyQt5-sip, lxml, pyqt5\n",
      "  Attempting uninstall: PyQt5-sip\n",
      "    Found existing installation: PyQt5-sip 12.13.0\n",
      "    Uninstalling PyQt5-sip-12.13.0:\n",
      "      Successfully uninstalled PyQt5-sip-12.13.0\n",
      "  Attempting uninstall: lxml\n",
      "    Found existing installation: lxml 5.2.1\n",
      "    Uninstalling lxml-5.2.1:\n",
      "      Successfully uninstalled lxml-5.2.1\n",
      "  Attempting uninstall: pyqt5\n",
      "    Found existing installation: PyQt5 5.15.10\n",
      "    Uninstalling PyQt5-5.15.10:\n",
      "      Successfully uninstalled PyQt5-5.15.10\n",
      "Successfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.17.0 lxml-5.3.1 pyqt5-5.15.11\n"
     ]
    }
   ],
   "source": [
    "!pip install pyqt5 lxml --upgrade\n",
    "!cd labelImg && pyrcc5 -o libs/resources.py resources.qrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13cee63-10b9-4263-bcce-58ad70d53975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONDict(\"C:\\Users\\BiswarajDey\\AppData\\Roaming\\Ultralytics\\settings.json\"):\n",
      "{\n",
      "  \"settings_version\": \"0.0.6\",\n",
      "  \"datasets_dir\": \"C:\\\\Users\\\\datasets\",\n",
      "  \"weights_dir\": \"C:\\\\Users\\\\BiswarajDey\\\\weights\",\n",
      "  \"runs_dir\": \"C:\\\\Users\\\\BiswarajDey\\\\runs\",\n",
      "  \"uuid\": \"6e97a6d22a7d03b278536ef8e3d7a10a76111988dd85b043ac40fa3ea7dc688d\",\n",
      "  \"sync\": true,\n",
      "  \"api_key\": \"\",\n",
      "  \"openai_api_key\": \"\",\n",
      "  \"clearml\": true,\n",
      "  \"comet\": true,\n",
      "  \"dvc\": true,\n",
      "  \"hub\": true,\n",
      "  \"mlflow\": true,\n",
      "  \"neptune\": true,\n",
      "  \"raytune\": true,\n",
      "  \"tensorboard\": true,\n",
      "  \"wandb\": false,\n",
      "  \"vscode_msg\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import settings\n",
    "\n",
    "# View all settings\n",
    "print(settings)\n",
    "\n",
    "# Return a specific setting\n",
    "value = settings[\"runs_dir\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42551685-a6c7-4b31-8ef8-ad2442278d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings\n",
    "\n",
    "# Update a setting\n",
    "settings.update({\"runs_dir\": \"/path/to/runs\"})\n",
    "\n",
    "# Update multiple settings\n",
    "settings.update({\"runs_dir\": \"/path/to/runs\", \"tensorboard\": False})\n",
    "\n",
    "# Reset settings to default values\n",
    "settings.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add103a6-f97b-46b7-b00a-8bc27f070940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mgithub: \u001b[0mskipping check (offline), for updates see https://github.com/ultralytics/yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=dataset.yaml, hyp=data\\hyps\\hyp.scratch-low.yaml, epochs=20, batch_size=16, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data\\hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=2, project=runs\\train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "fatal: cannot change to 'C:\\Users\\BiswarajDey\\My': No such file or directory\n",
      "YOLOv5  2025-3-23 Python-3.12.7 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5  runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     18879  models.yolo.Detect                      [2, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7025023 parameters, 7025023 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\yolov5\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\yolov5\\models\\common.py:894: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\data\\labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|##########| 40/40 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\data\\labels.cache... 40 images, 0 backgrounds, 0 corrupt: 100%|##########| 40/40 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\yolov5\\train.py\", line 986, in <module>\n",
      "    main(opt)\n",
      "  File \"C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\yolov5\\train.py\", line 688, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"C:\\Users\\BiswarajDey\\My Stuff\\Biswa VS CODE\\Deep Learning Projects\\Driver Drowsiness\\yolov5\\train.py\", line 305, in train\n",
      "    assert mlc < nc, f\"Label class {mlc} exceeds nc={nc} in {data}. Possible class labels are 0-{nc - 1}\"\n",
      "           ^^^^^^^^\n",
      "AssertionError: Label class 16 exceeds nc=2 in dataset.yaml. Possible class labels are 0-1\n"
     ]
    }
   ],
   "source": [
    "!cd yolov5 && python train.py --img 320 --batch 16 --epochs 20 --data dataset.yaml --weights yolov5s.pt --workers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22611ba8-01ff-46c1-b431-6a39f0b5d43b",
   "metadata": {
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\BiswarajDey/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-3-22 Python-3.12.7 torch-2.6.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:70\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m     model \u001b[38;5;241m=\u001b[39m DetectMultiBackend(path, device\u001b[38;5;241m=\u001b[39mdevice, fuse\u001b[38;5;241m=\u001b[39mautoshape)  \u001b[38;5;66;03m# detection model\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m autoshape:\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:489\u001b[0m, in \u001b[0;36mDetectMultiBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, fuse)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m     model \u001b[38;5;241m=\u001b[39m attempt_load(weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m w, device\u001b[38;5;241m=\u001b[39mdevice, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m    490\u001b[0m     stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax()), \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# model stride\u001b[39;00m\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(attempt_download(w), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1471\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   1474\u001b[0m             pickle_module,\n\u001b[0;32m   1475\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:85\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m         model \u001b[38;5;241m=\u001b[39m attempt_load(path, device\u001b[38;5;241m=\u001b[39mdevice, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# arbitrary model\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\experimental.py:98\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, device, inplace, fuse)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[1;32m---> 98\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(attempt_download(w), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\ultralytics\\utils\\patches.py:86\u001b[0m, in \u001b[0;36mtorch_load\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_only\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _torch_load(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1471\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[0;32m   1474\u001b[0m             pickle_module,\n\u001b[0;32m   1475\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pathlib.py:1422\u001b[0m, in \u001b[0;36mPosixPath.__new__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m-> 1422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot instantiate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m on your system\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: cannot instantiate 'PosixPath' on your system",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics/yolov5\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m'\u001b[39m,path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5/runs/train/exp2/weights/last.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\hub.py:647\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m model \u001b[38;5;241m=\u001b[39m _load_local(repo_or_dir, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\hub.py:676\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    675\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 676\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:135\u001b[0m, in \u001b[0;36mcustom\u001b[1;34m(path, autoshape, _verbose, device)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, autoshape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, _verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m    Loads a custom or local YOLOv5 model from a given path with optional autoshaping and device specification.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;124;03m        ```\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _create(path, autoshape\u001b[38;5;241m=\u001b[39mautoshape, verbose\u001b[38;5;241m=\u001b[39m_verbose, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\ultralytics_yolov5_master\\hubconf.py:103\u001b[0m, in \u001b[0;36m_create\u001b[1;34m(name, pretrained, channels, classes, autoshape, verbose, device)\u001b[0m\n\u001b[0;32m    101\u001b[0m help_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Cache may be out of date, try `force_reload=True` or see \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhelp_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for help.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(s) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: cannot instantiate 'PosixPath' on your system. Cache may be out of date, try `force_reload=True` or see https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading for help."
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5','custom',path='yolov5/runs/train/exp2/weights/last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924317bb-a917-4c5d-8d22-782467de72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = os.path.join('data', 'images', 'drowsy.14221e5e-07a3-11f0-a916-48684a707730.jpg')\n",
    "results = model(img)\n",
    "print(results.print())\n",
    "\n",
    "%matplotlib inline \n",
    "plt.imshow(np.squeeze(results.render()))\n",
    "print(plt.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afa8715-faed-4f4c-99c6-95dd1d79c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Make detections \n",
    "    results = model(frame)\n",
    "    \n",
    "    cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691d9c97",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc07507-ba15-4eae-9ced-57027141b128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1c249-1dcd-42ee-b805-67981e146ec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40c9f2-2107-4da7-9b07-ee4e8f7a05d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f8dda-1267-4083-a076-eaadff43e6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492064f1-9f1a-4e49-a4a3-11622bca6ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b04f7cd-1f12-4c97-917e-8448a1e47ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cb692-ff9f-4e85-8ac2-d25198c9c8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608d021-ec7d-44be-b775-37317c152981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2ef4a6e-21dd-48e1-8231-3e9cd011a620",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m     \tcv2\u001b[38;5;241m.\u001b[39mcircle(face_frame, (x, y), \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     98\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame)\n\u001b[1;32m---> 99\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResult of detector\u001b[39m\u001b[38;5;124m\"\u001b[39m, face_frame)\n\u001b[0;32m    100\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_frame' is not defined"
     ]
    }
   ],
   "source": [
    "#Importing OpenCV Library for basic image processing functions\n",
    "import cv2\n",
    "# Numpy for array related functions\n",
    "import numpy as np\n",
    "# Dlib for deep learning based Modules and face landmark detection\n",
    "import dlib\n",
    "#face_utils for basic operations of conversion\n",
    "from imutils import face_utils\n",
    "\n",
    "\n",
    "#Initializing the camera and taking the instance\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Initializing the face detector and landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#status marking for current state\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "status=\"\"\n",
    "color=(0,0,0)\n",
    "\n",
    "def compute(ptA,ptB):\n",
    "\tdist = np.linalg.norm(ptA - ptB)\n",
    "\treturn dist\n",
    "\n",
    "def blinked(a,b,c,d,e,f):\n",
    "\tup = compute(b,d) + compute(c,e)\n",
    "\tdown = compute(a,f)\n",
    "\tratio = up/(2.0*down)\n",
    "\n",
    "\t#Checking if it is blinked\n",
    "\tif(ratio>0.25):\n",
    "\t\treturn 2\n",
    "\telif(ratio>0.21 and ratio<=0.25):\n",
    "\t\treturn 1\n",
    "\telse:\n",
    "\t\treturn 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = detector(gray)\n",
    "    #detected face in faces array\n",
    "    for face in faces:\n",
    "        x1 = face.left()\n",
    "        y1 = face.top()\n",
    "        x2 = face.right()\n",
    "        y2 = face.bottom()\n",
    "\n",
    "        face_frame = frame.copy()\n",
    "        cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        #The numbers are actually the landmarks which will show eye\n",
    "        left_blink = blinked(landmarks[36],landmarks[37], \n",
    "        \tlandmarks[38], landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42],landmarks[43], \n",
    "        \tlandmarks[44], landmarks[47], landmarks[46], landmarks[45])\n",
    "        \n",
    "        #Now judge what to do for the eye blinks\n",
    "        if(left_blink==0 or right_blink==0):\n",
    "        \tsleep+=1\n",
    "        \tdrowsy=0\n",
    "        \tactive=0\n",
    "        \tif(sleep>6):\n",
    "        \t\tstatus=\"SLEEPING !!!\"\n",
    "        \t\tcolor = (255,0,0)\n",
    "\n",
    "        elif(left_blink==1 or right_blink==1):\n",
    "        \tsleep=0\n",
    "        \tactive=0\n",
    "        \tdrowsy+=1\n",
    "        \tif(drowsy>6):\n",
    "        \t\tstatus=\"Drowsy !\"\n",
    "        \t\tcolor = (0,0,255)\n",
    "\n",
    "        else:\n",
    "        \tdrowsy=0\n",
    "        \tsleep=0\n",
    "        \tactive+=1\n",
    "        \tif(active>6):\n",
    "        \t\tstatus=\"Active :)\"\n",
    "        \t\tcolor = (0,255,0)\n",
    "        \t\n",
    "        cv2.putText(frame, status, (100,100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color,3)\n",
    "\n",
    "        for n in range(0, 68):\n",
    "        \t(x,y) = landmarks[n]\n",
    "        \tcv2.circle(face_frame, (x, y), 1, (255, 255, 255), -1)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Result of detector\", face_frame)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "      \tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893f706a-3e28-4954-8d21-dc6a9971e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenCV Library for basic image processing functions\n",
    "import cv2\n",
    "# Numpy for array related functions\n",
    "import numpy as np\n",
    "# Dlib for deep learning-based Modules and face landmark detection\n",
    "import dlib\n",
    "# face_utils for basic operations of conversion\n",
    "from imutils import face_utils\n",
    "\n",
    "# Initializing the camera and taking the instance\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing the face detector and landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Status marking for current state\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "status = \"\"\n",
    "color = (0, 0, 0)\n",
    "\n",
    "def compute(ptA, ptB):\n",
    "    dist = np.linalg.norm(ptA - ptB)\n",
    "    return dist\n",
    "\n",
    "def blinked(a, b, c, d, e, f):\n",
    "    up = compute(b, d) + compute(c, e)\n",
    "    down = compute(a, f)\n",
    "    ratio = up / (2.0 * down)\n",
    "\n",
    "    # Checking if it is blinked\n",
    "    if ratio > 0.25:\n",
    "        return 2\n",
    "    elif 0.21 <= ratio <= 0.25:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Ensure face_frame is always defined\n",
    "    face_frame = frame.copy()\n",
    "\n",
    "    faces = detector(gray)\n",
    "    # Detected face in faces array\n",
    "    for face in faces:\n",
    "        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "        cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        # The numbers are actually the landmarks which will show eye\n",
    "        left_blink = blinked(landmarks[36], landmarks[37], \n",
    "                             landmarks[38], landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42], landmarks[43], \n",
    "                              landmarks[44], landmarks[47], landmarks[46], landmarks[45])\n",
    "        \n",
    "        # Now judge what to do for the eye blinks\n",
    "        if left_blink == 0 or right_blink == 0:\n",
    "            sleep += 1\n",
    "            drowsy = 0\n",
    "            active = 0\n",
    "            if sleep > 6:\n",
    "                status = \"SLEEPING !!!\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "        elif left_blink == 1 or right_blink == 1:\n",
    "            sleep = 0\n",
    "            active = 0\n",
    "            drowsy += 1\n",
    "            if drowsy > 6:\n",
    "                status = \"Drowsy !\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "        else:\n",
    "            drowsy = 0\n",
    "            sleep = 0\n",
    "            active += 1\n",
    "            if active > 6:\n",
    "                status = \"Active :)\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, status, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(face_frame, (x, y), 1, (255, 255, 255), -1)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Result of detector\", face_frame)  # No more NameError\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "438fb5c1-daad-4512-8ab9-20bce46613c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing OpenCV Library for basic image processing functions\n",
    "import cv2\n",
    "# Numpy for array related functions\n",
    "import numpy as np\n",
    "# Dlib for deep learning-based Modules and face landmark detection\n",
    "import dlib\n",
    "# face_utils for basic operations of conversion\n",
    "from imutils import face_utils\n",
    "\n",
    "# Initializing the camera and taking the instance\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing the face detector and landmark detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Status marking for current state\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "awake_level = 0.5  # Starts at a neutral level (0.5 means neither drowsy nor fully awake)\n",
    "status = \"\"\n",
    "color = (0, 0, 0)\n",
    "\n",
    "def compute(ptA, ptB):\n",
    "    return np.linalg.norm(ptA - ptB)\n",
    "\n",
    "def blinked(a, b, c, d, e, f):\n",
    "    up = compute(b, d) + compute(c, e)\n",
    "    down = compute(a, f)\n",
    "    ratio = up / (2.0 * down)\n",
    "\n",
    "    # Checking if it is blinked\n",
    "    if ratio > 0.25:\n",
    "        return 2\n",
    "    elif 0.21 <= ratio <= 0.25:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Ensure face_frame is always defined\n",
    "    face_frame = frame.copy()\n",
    "\n",
    "    faces = detector(gray)\n",
    "    # Detected face in faces array\n",
    "    for face in faces:\n",
    "        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()\n",
    "        cv2.rectangle(face_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        # The numbers are actually the landmarks which will show eye\n",
    "        left_blink = blinked(landmarks[36], landmarks[37], \n",
    "                             landmarks[38], landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42], landmarks[43], \n",
    "                              landmarks[44], landmarks[47], landmarks[46], landmarks[45])\n",
    "        \n",
    "        # Now judge what to do for the eye blinks\n",
    "        if left_blink == 0 or right_blink == 0:\n",
    "            sleep += 1\n",
    "            drowsy = 0\n",
    "            active = 0\n",
    "            if sleep > 6:\n",
    "                status = \"SLEEPING !!!\"\n",
    "                color = (255, 0, 0)\n",
    "                awake_level = max(0, awake_level - 0.05)  # Decrease awake level\n",
    "\n",
    "        elif left_blink == 1 or right_blink == 1:\n",
    "            sleep = 0\n",
    "            active = 0\n",
    "            drowsy += 1\n",
    "            if drowsy > 6:\n",
    "                status = \"Drowsy !\"\n",
    "                color = (0, 0, 255)\n",
    "                awake_level = max(0, awake_level - 0.02)  # Slight decrease in awake level\n",
    "\n",
    "        else:\n",
    "            drowsy = 0\n",
    "            sleep = 0\n",
    "            active += 1\n",
    "            if active > 6:\n",
    "                status = \"Active :)\"\n",
    "                color = (0, 255, 0)\n",
    "                awake_level = min(1, awake_level + 0.05)  # Increase awake level\n",
    "\n",
    "        cv2.putText(frame, status, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(face_frame, (x, y), 1, (255, 255, 255), -1)\n",
    "\n",
    "    # Draw the awake level as a progress bar\n",
    "    bar_length = int(awake_level * 300)  # Scale to 300 pixels\n",
    "    cv2.rectangle(frame, (50, 400), (50 + bar_length, 430), (0, 255, 0), -1)\n",
    "    cv2.rectangle(frame, (50, 400), (350, 430), (255, 255, 255), 2)  # Progress bar border\n",
    "\n",
    "    # Display awake level as text\n",
    "    cv2.putText(frame, f\"Awake Level: {awake_level:.2f}\", (50, 390),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Result of detector\", face_frame)  # No more NameError\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab3cb83-4285-42b2-96f3-7c8faa7e3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame  # Import pygame for audio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import threading\n",
    "\n",
    "# Initialize pygame mixer for sound\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"alarm.wav\")  # Load alarm sound\n",
    "\n",
    "# Initializing the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing the face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Status variables\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "status = \"\"\n",
    "color = (0, 0, 0)\n",
    "alarm_playing = False\n",
    "\n",
    "def compute(ptA, ptB):\n",
    "    return np.linalg.norm(ptA - ptB)\n",
    "\n",
    "def blinked(a, b, c, d, e, f):\n",
    "    up = compute(b, d) + compute(c, e)\n",
    "    down = compute(a, f)\n",
    "    ratio = up / (2.0 * down)\n",
    "    \n",
    "    if ratio > 0.25:\n",
    "        return 2\n",
    "    elif 0.21 <= ratio <= 0.25:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def play_alarm():\n",
    "    \"\"\"Plays an alarm sound (runs in a separate thread).\"\"\"\n",
    "    global alarm_playing\n",
    "    if not alarm_playing:\n",
    "        alarm_playing = True\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():  # Wait until sound finishes\n",
    "            continue\n",
    "        alarm_playing = False  # Reset after playing\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    eye_frame = np.zeros_like(frame)  # Blank frame for eye tracking\n",
    "    landmarks_frame = frame.copy()  # Copy frame for landmark visualization\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        left_blink = blinked(landmarks[36], landmarks[37], landmarks[38], \n",
    "                             landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42], landmarks[43], landmarks[44], \n",
    "                              landmarks[47], landmarks[46], landmarks[45])\n",
    "\n",
    "        # Extract left and right eye regions\n",
    "        left_eye_pts = landmarks[36:42]\n",
    "        right_eye_pts = landmarks[42:48]\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        cv2.polylines(frame, [left_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "        cv2.polylines(frame, [right_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "\n",
    "        # Crop and resize the eyes\n",
    "        x1, y1 = np.min(left_eye_pts, axis=0)\n",
    "        x2, y2 = np.max(left_eye_pts, axis=0)\n",
    "        left_eye_roi = gray[y1:y2, x1:x2]\n",
    "\n",
    "        x1, y1 = np.min(right_eye_pts, axis=0)\n",
    "        x2, y2 = np.max(right_eye_pts, axis=0)\n",
    "        right_eye_roi = gray[y1:y2, x1:x2]\n",
    "\n",
    "        # Resize for display\n",
    "        left_eye_display = cv2.resize(left_eye_roi, (150, 80))\n",
    "        right_eye_display = cv2.resize(right_eye_roi, (150, 80))\n",
    "\n",
    "        # Place in eye tracking window\n",
    "        eye_frame[50:130, 50:200] = cv2.cvtColor(left_eye_display, cv2.COLOR_GRAY2BGR)\n",
    "        eye_frame[50:130, 250:400] = cv2.cvtColor(right_eye_display, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Draw white dots for all 68 landmarks\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(landmarks_frame, (x, y), 2, (255, 255, 255), -1)  # White dots\n",
    "\n",
    "        if left_blink == 0 or right_blink == 0:\n",
    "            sleep += 1\n",
    "            drowsy = 0\n",
    "            active = 0\n",
    "            if sleep > 6:\n",
    "                status = \"SLEEPING !!!\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        elif left_blink == 1 or right_blink == 1:\n",
    "            sleep = 0\n",
    "            active = 0\n",
    "            drowsy += 1\n",
    "            if drowsy > 6:\n",
    "                status = \"Drowsy !\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        else:\n",
    "            drowsy = 0\n",
    "            sleep = 0\n",
    "            active += 1\n",
    "            if active > 6:\n",
    "                status = \"Active :)\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, status, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "\n",
    "    # Show main frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Eye Tracking\", eye_frame)  # Show eye tracking window\n",
    "    cv2.imshow(\"Face Landmarks\", landmarks_frame)  # Show face with white dots\n",
    "\n",
    "    if cv2.waitKey(1) == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()  # Clean up pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c8cd042-01d1-4ba2-8181-783ca424f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame  # Import pygame for audio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import threading\n",
    "\n",
    "# Initialize pygame mixer for sound\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"alarm.wav\")  # Load alarm sound\n",
    "\n",
    "# Initializing the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing the face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Status variables\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "status = \"\"\n",
    "color = (0, 0, 0)\n",
    "alarm_playing = False\n",
    "\n",
    "wakefulness_level = 0.0  # Variable to store wakefulness level\n",
    "\n",
    "def compute(ptA, ptB):\n",
    "    return np.linalg.norm(ptA - ptB)\n",
    "\n",
    "def blinked(a, b, c, d, e, f):\n",
    "    up = compute(b, d) + compute(c, e)\n",
    "    down = compute(a, f)\n",
    "    ratio = up / (2.0 * down)\n",
    "    \n",
    "    if ratio > 0.25:\n",
    "        return 2\n",
    "    elif 0.21 <= ratio <= 0.25:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def play_alarm():\n",
    "    \"\"\"Plays an alarm sound (runs in a separate thread).\"\"\"\n",
    "    global alarm_playing\n",
    "    if not alarm_playing:\n",
    "        alarm_playing = True\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():  # Wait until sound finishes\n",
    "            continue\n",
    "        alarm_playing = False  # Reset after playing\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    eye_frame = np.zeros_like(frame)  # Blank frame for eye tracking\n",
    "    landmarks_frame = frame.copy()  # Copy frame for landmark visualization\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        left_blink = blinked(landmarks[36], landmarks[37], landmarks[38], \n",
    "                             landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42], landmarks[43], landmarks[44], \n",
    "                              landmarks[47], landmarks[46], landmarks[45])\n",
    "\n",
    "        # Extract left and right eye regions\n",
    "        left_eye_pts = landmarks[36:42]\n",
    "        right_eye_pts = landmarks[42:48]\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        cv2.polylines(frame, [left_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "        cv2.polylines(frame, [right_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "\n",
    "        # Draw white dots for all 68 landmarks\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(landmarks_frame, (x, y), 2, (255, 255, 255), -1)  # White dots\n",
    "\n",
    "        if left_blink == 0 or right_blink == 0:\n",
    "            sleep += 1\n",
    "            drowsy = 0\n",
    "            active = 0\n",
    "            wakefulness_level = 0.0  # Fully asleep\n",
    "            if sleep > 6:\n",
    "                status = \"SLEEPING !!!\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        elif left_blink == 1 or right_blink == 1:\n",
    "            sleep = 0\n",
    "            active = 0\n",
    "            drowsy += 1\n",
    "            wakefulness_level = 0.5  # Drowsy state\n",
    "            if drowsy > 6:\n",
    "                status = \"Drowsy !\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        else:\n",
    "            drowsy = 0\n",
    "            sleep = 0\n",
    "            active += 1\n",
    "            wakefulness_level = 1.0  # Fully awake\n",
    "            if active > 6:\n",
    "                status = \"Active :)\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, status, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "        cv2.putText(frame, f\"Wakefulness: {wakefulness_level:.2f}\", (100, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "\n",
    "    # Show main frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Eye Tracking\", eye_frame)  # Show eye tracking window\n",
    "    cv2.imshow(\"Face Landmarks\", landmarks_frame)  # Show face with white dots\n",
    "\n",
    "    if cv2.waitKey(1) == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()  # Clean up pygame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2824d155-b11b-457c-9f26-e4eedb3fb95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 1\n",
      "Left EAR: 2, Right EAR: 1\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 2, Right EAR: 2\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 0, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 0\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 1, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 1\n",
      "Left EAR: 0, Right EAR: 0\n"
     ]
    }
   ],
   "source": [
    "import pygame  # Import pygame for audio\n",
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "import threading\n",
    "\n",
    "# Initialize pygame mixer for sound\n",
    "pygame.mixer.init()\n",
    "pygame.mixer.music.load(\"alarm.wav\")  # Load alarm sound\n",
    "\n",
    "# Initializing the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initializing the face detector and landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Status variables\n",
    "sleep = 0\n",
    "drowsy = 0\n",
    "active = 0\n",
    "status = \"\"\n",
    "color = (0, 0, 0)\n",
    "alarm_playing = False\n",
    "\n",
    "wakefulness_level = 0.0  # Variable to store wakefulness level\n",
    "\n",
    "def compute(ptA, ptB):\n",
    "    return np.linalg.norm(ptA - ptB)\n",
    "\n",
    "def blinked(a, b, c, d, e, f):\n",
    "    up = compute(b, d) + compute(c, e)\n",
    "    down = compute(a, f)\n",
    "    ratio = up / (2.0 * down)\n",
    "    \n",
    "    if ratio > 0.26:\n",
    "        return 2\n",
    "    elif 0.18 <= ratio <= 0.26:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def play_alarm():\n",
    "    \"\"\"Plays an alarm sound (runs in a separate thread).\"\"\"\n",
    "    global alarm_playing\n",
    "    if not alarm_playing:\n",
    "        alarm_playing = True\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():  # Wait until sound finishes\n",
    "            continue\n",
    "        alarm_playing = False  # Reset after playing\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    eye_frame = np.zeros_like(frame)  # Blank frame for eye tracking\n",
    "    landmarks_frame = frame.copy()  # Copy frame for landmark visualization\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        landmarks = face_utils.shape_to_np(landmarks)\n",
    "\n",
    "        left_blink = blinked(landmarks[36], landmarks[37], landmarks[38], \n",
    "                             landmarks[41], landmarks[40], landmarks[39])\n",
    "        right_blink = blinked(landmarks[42], landmarks[43], landmarks[44], \n",
    "                              landmarks[47], landmarks[46], landmarks[45])\n",
    "\n",
    "        # Extract left and right eye regions\n",
    "        left_eye_pts = landmarks[36:42]\n",
    "        right_eye_pts = landmarks[42:48]\n",
    "\n",
    "        # Draw contours around eyes\n",
    "        cv2.polylines(frame, [left_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "        cv2.polylines(frame, [right_eye_pts], isClosed=True, color=(255, 255, 0), thickness=1)\n",
    "\n",
    "        # Draw white dots for all 68 landmarks\n",
    "        for (x, y) in landmarks:\n",
    "            cv2.circle(landmarks_frame, (x, y), 2, (255, 255, 255), -1)  # White dots\n",
    "\n",
    "        print(f\"Left EAR: {left_blink}, Right EAR: {right_blink}\")  # Debugging EAR values\n",
    "\n",
    "        if left_blink == 0 or right_blink == 0:\n",
    "            sleep += 1\n",
    "            drowsy = 0\n",
    "            active = 0\n",
    "            wakefulness_level = 0.0  # Fully asleep\n",
    "            if sleep > 5:\n",
    "                status = \"SLEEPING !!!\"\n",
    "                color = (255, 0, 0)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        elif left_blink == 1 or right_blink == 1:\n",
    "            sleep = 0\n",
    "            active = 0\n",
    "            drowsy += 1\n",
    "            wakefulness_level = 0.5  # Drowsy state\n",
    "            if drowsy > 4:\n",
    "                status = \"Drowsy !\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "                if not alarm_playing:\n",
    "                    threading.Thread(target=play_alarm, daemon=True).start()\n",
    "\n",
    "        else:\n",
    "            drowsy = 0\n",
    "            sleep = 0\n",
    "            active += 1\n",
    "            wakefulness_level = 1.0  # Fully awake\n",
    "            if active > 6:\n",
    "                status = \"Active :)\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "        cv2.putText(frame, status, (100, 100), cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)\n",
    "        cv2.putText(frame, f\"Wakefulness: {wakefulness_level:.2f}\", (100, 150), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2)\n",
    "\n",
    "    # Show main frame\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    cv2.imshow(\"Eye Tracking\", eye_frame)  # Show eye tracking window\n",
    "    cv2.imshow(\"Face Landmarks\", landmarks_frame)  # Show face with white dots\n",
    "\n",
    "    if cv2.waitKey(1) == 27:  # Press 'Esc' to exit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pygame.mixer.quit()  # Clean up pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc23630-f0d6-4198-97b6-2d8939a6cb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
